{"cells":[{"cell_type":"markdown","metadata":{"id":"D4YiAhK6JKO5"},"source":["# Overfitting e regolarizzazione\n","L'overfitting è un problema tipico del machine learning che si manifesta quando un modello si lega troppo ai dati di addestramento e fallisce nel generalizzare su dati nuovi.\n","\n","L'overffiting è caratterizzato da:\n","* **Alta variaza**: le previsioni per modelli addestrati con diverse parti del dataset saranno molto diverse tra loro.\n","* **Basso bias**: l'errore per le predizioni sul set di addestramento è mediamente molto basso"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"5B33xl57JKPB","executionInfo":{"status":"ok","timestamp":1730716784590,"user_tz":-60,"elapsed":8538,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"cTDGcY4nJKPD","executionInfo":{"status":"ok","timestamp":1730716796636,"user_tz":-60,"elapsed":1131,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["path = \"https://frenzy86.s3.eu-west-2.amazonaws.com/IFAO/boston_houses.csv\"\n","df = pd.read_csv(path)"]},{"cell_type":"code","source":["df.rename(columns={'MEDV':'Price'},inplace=True)"],"metadata":{"id":"QY_TwfpRK0aN","executionInfo":{"status":"ok","timestamp":1730716796949,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"78EV2B8KJKPF","executionInfo":{"status":"ok","timestamp":1730716798392,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["target = 'Price'\n","X = df.drop(target,axis=1).values\n","y = df[target].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X,y,\n","                                                    test_size=0.3,\n","                                                    random_state=667,\n","                                                    )"]},{"cell_type":"markdown","metadata":{"id":"HHcK5SE2JKPF"},"source":["### Creiamo le features polinomiali\n","Per correggere l'overfitting prima dobbiamo causarlo, un buon modo è aumentare la complessità del nostro modello aumentando il numero di features utilizzando i polinomi."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Azf5yd5LJKPG","executionInfo":{"status":"ok","timestamp":1730716823173,"user_tz":-60,"elapsed":285,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"ee12d12f-5ba3-46bb-fd7f-bca56bca8e95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Numero di esempi nel test: 354\n","Numero di features: 105\n"]}],"source":["polyfeats = PolynomialFeatures(degree=2)\n","X_train_poly = polyfeats.fit_transform(X_train)\n","X_test_poly = polyfeats.transform(X_test)\n","\n","print(\"Numero di esempi nel test: \"+str(X_train_poly.shape[0]))\n","print(\"Numero di features: \"+str(X_train_poly.shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"FiVon-mTJKPG"},"source":["### Standardizziamo i dati\n","**NOTA BENE** Per applicare la regolarizzazione è sempre necessario portare i dati sulla stessa scala."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"arRzTaScJKPH","executionInfo":{"status":"ok","timestamp":1730716884904,"user_tz":-60,"elapsed":292,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["ss = StandardScaler()\n","X_train_poly = ss.fit_transform(X_train_poly)\n","X_test_poly = ss.transform(X_test_poly)"]},{"cell_type":"markdown","metadata":{"id":"ewdgmsYIJKPH"},"source":["Adesso il nostro set di addestramento contiene 354 e 105 features, abbastanza complesso !"]},{"cell_type":"markdown","metadata":{"id":"O7n_gHcJJKPI"},"source":["### Riconoscere l'overfitting\n","Evidenziare un problema di overfitting è molto semplice, un modello che ne soffre avrà memorizzato la struttura dei dati di addestramento, piuttosto che imparare da essi, quindi l'errore per le predizioni sul train set sarà molto basso, invece fallirà nel generalizzare, perciò l'errore nel test set sarà decisamente più alto.<br><br>\n","Quindi per riconoscere l'overfitting è sufficente confrontare questi due valori, scriviamo una funzione che ci permette di farlo in modo da non dover scrivere più volte lo stesso codice."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"OCcGJzijJKPI","executionInfo":{"status":"ok","timestamp":1730716892653,"user_tz":-60,"elapsed":286,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["def overfit_eval(model, X, y):\n","\n","    \"\"\"\n","    model: il nostro modello predittivo già addestrato\n","    X: una tupla contenente le prorietà del train set e test set (X_train, X_test)\n","    y: una tupla contenente target del train set e test set (y_train, y_test)\n","    \"\"\"\n","\n","    y_pred_train = model.predict(X[0])\n","    y_pred_test = model.predict(X[1])\n","\n","    mse_train = mean_squared_error(y[0], y_pred_train)\n","    mse_test = mean_squared_error(y[1], y_pred_test)\n","\n","    r2_train = r2_score(y[0], y_pred_train)\n","    r2_test = r2_score(y[1], y_pred_test)\n","\n","    print(\"Train set:  MSE=\"+str(mse_train)+\" R2=\"+str(r2_train))\n","    print(\"Test set:  MSE=\"+str(mse_test)+\" R2=\"+str(r2_test))"]},{"cell_type":"markdown","metadata":{"id":"v4kA95nbJKPI"},"source":["### Regressione lineare non regolarizzata\n","Cominciamo eseguendo una regressione lineare (in realtà si tratta di una regressione polinomiale) senza applicare la regolarizzazione."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtEOSxwgJKPJ","executionInfo":{"status":"ok","timestamp":1730716934429,"user_tz":-60,"elapsed":275,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"cba6aa7a-c068-4091-be72-4211412088dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train set:  MSE=15.034347840028595 R2=0.8179241488460587\n","Test set:  MSE=32.03088973402763 R2=0.6389761364103487\n"]}],"source":["ll = LinearRegression()\n","ll.fit(X_train_poly, y_train)\n","\n","overfit_eval(ll, (X_train_poly, X_test_poly),(y_train, y_test))"]},{"cell_type":"markdown","metadata":{"id":"Ej5FHarhJKPJ"},"source":["Il modello predice in maniera estremamente (o meglio dire eccessivamente) accurata i dati del train set, mentre è molto più scarso sul test set. Siamo di fronte ad un caso di overfitting."]},{"cell_type":"markdown","metadata":{"id":"JGAsfD6ZJKPK"},"source":["## Regolarizzazione L2: Ridge Regression\n","La ridge regression è un modello di regressione lineare che applica la **regolarizzazione L2**, la quale consiste nell'aggiungere una penalità per i pesi nella funzione di costo durante la fase di addestramento.<br>\n","La penalità è data dalla somma dei quadrati dei pesi:\n","$$\\lambda\\sum_{j=1}^{M}W_j^2$$<br>\n","**Lambda** (conosciuto anche come **alpha**) è il **parametro di regolarizzazione** ed è un'altro iperparametro.\n","Eseguiamo diverse Ridge regression per diversi valori di alpha."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwcfWgA_JKPK","executionInfo":{"status":"ok","timestamp":1730717156392,"user_tz":-60,"elapsed":288,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"e27f3ea6-1a44-44ad-8a75-9033e09368e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha=0.0001\n","Train set:  MSE=3.994688160485263 R2=0.9516216962216072\n","Test set:  MSE=15.152641036952769 R2=0.8292128299846662\n","Alpha=0.001\n","Train set:  MSE=4.008703827485975 R2=0.9514519572661302\n","Test set:  MSE=15.124700794392753 R2=0.8295277476907432\n","Alpha=0.01\n","Train set:  MSE=4.058274133283384 R2=0.9508516282251861\n","Test set:  MSE=15.105305007020014 R2=0.8297463598539649\n","Alpha=0.1\n","Train set:  MSE=4.402406182868336 R2=0.9466839625236713\n","Test set:  MSE=15.48999674799777 R2=0.8254104547394941\n","Alpha=1\n","Train set:  MSE=5.509493608081705 R2=0.9332764048835013\n","Test set:  MSE=16.523397957976183 R2=0.8137628701559074\n","Alpha=10\n","Train set:  MSE=8.278399484203565 R2=0.8997431316398327\n","Test set:  MSE=20.189297287953412 R2=0.7724440947291633\n"]}],"source":["from sklearn.linear_model import Ridge\n","\n","alphas = [0.0001, 0.001, 0.01, 0.1 ,1 ,10] #alpha corrispone a lambda\n","\n","for alpha in alphas:\n","    print(\"Alpha=\"+str(alpha))\n","    ridge = Ridge(alpha=alpha)\n","    ridge.fit(X_train_poly, y_train)\n","\n","    overfit_eval(ridge, (X_train_poly, X_test_poly),(y_train, y_test))"]},{"cell_type":"markdown","metadata":{"id":"yjGSBfJ3JKPK"},"source":["La Ridge regression, applicando la regolarizzazione L2, ci permette di ridurre l'overfitting e portare l'R2 fino ad un valore di 0.791 per alpha uguale a 10."]},{"cell_type":"markdown","metadata":{"id":"Yz4yo-7LJKPK"},"source":["## Regolarizzazione L1: Lasso\n","Lasso è un modello di regressione lineare che applica la regolarizzazione L1, questa funziona in egual modo alla L2, con la differenza che il termine di regolarizza sarà dato dalla somma del valore assoluto dei pesi:\n","$$\\lambda\\sum_{j=1}^{M}|W_j|$$<br>\n","e viene sempre applicato alla funzione di costo durante la fase di addestramento"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cM1thKxcJKPL","executionInfo":{"status":"ok","timestamp":1730717285507,"user_tz":-60,"elapsed":282,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"48e3634f-8a61-496d-b194-96480d8cf1a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha=0.0001\n","Train set:  MSE=4.879663235273139 R2=0.9409040653867656\n","Test set:  MSE=17.58861506387127 R2=0.8017566849289173\n","Alpha=0.001\n","Train set:  MSE=4.909409855251361 R2=0.9405438142332658\n","Test set:  MSE=17.42818482600246 R2=0.8035649126988277\n","Alpha=0.01\n","Train set:  MSE=5.916971830313482 R2=0.9283415753232851\n","Test set:  MSE=17.03221744386145 R2=0.8080279068692382\n","Alpha=0.1\n","Train set:  MSE=11.021977307534174 R2=0.8665165977917872\n","Test set:  MSE=25.793165540963034 R2=0.7092822473827975\n","Alpha=1\n","Train set:  MSE=18.703798184263565 R2=0.7734846891632877\n","Test set:  MSE=35.19362698600457 R2=0.603328558971542\n","Alpha=10\n","Train set:  MSE=82.57189377254302 R2=0.0\n","Test set:  MSE=88.72279474960983 R2=-4.883253304388546e-06\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.614e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.387e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.399e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n"]}],"source":["from sklearn.linear_model import Lasso\n","\n","alphas = [0.0001, 0.001, 0.01, 0.1 ,1 ,10] #alpha corrisponde a lambda\n","\n","for alpha in alphas:\n","    print(\"Alpha=\"+str(alpha))\n","    lasso = Lasso(alpha=alpha)\n","    lasso.fit(X_train_poly, y_train)\n","\n","    overfit_eval(lasso, (X_train_poly, X_test_poly),(y_train, y_test))"]},{"cell_type":"markdown","metadata":{"id":"55jOjXXFJKPL"},"source":["Lasso ci permette di ottenere un modello ancora migliore, con un R2 di 0.803 per Lambda uguale a 0.1.<br>\n","Da notare che per valori di lambda più grandi il modello peggiora, questo perché l'effetto della regolarizzazione sarà molto pesante e buona parte dei pesi saranno portati a 0."]},{"cell_type":"markdown","metadata":{"id":"A2nqMe4ZJKPL"},"source":["## L2 ed L1 insieme: ElasticNet\n","ElasticNet è un modello di regressione lineare che implementa entrambe le tecniche di regolarizzazone L2 ed L1.<br>\n","Tramite il parametro <span style=\"font-family: Monaco\">l1_ration</span> possiamo controllare l'effetto delle due regolarizzazione\n"," * **<span style=\"font-family: Monaco\">l1_ration>0.5</span>** l'effetto della regolarizzazione L1 sarà più intenso rispetto alla L2.\n"," * **<span style=\"font-family: Monaco\">l1_ration<0.5</span>** l'effetto della regolarizzazione L2 sarà più intenso rispetto alla L1."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4_WOh7rJKPL","executionInfo":{"status":"ok","timestamp":1730717373368,"user_tz":-60,"elapsed":288,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"85a80ad1-e5ec-4c34-948b-3729db06b88e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Lambda is: 0.0001\n","Train set:  MSE=4.885168469233903 R2=0.9408373933787827\n","Test set:  MSE=17.536105176623664 R2=0.8023485299425339\n","Lambda is: 0.001\n","Train set:  MSE=5.022774408558441 R2=0.9391708948520129\n","Test set:  MSE=17.0500135999814 R2=0.8078273243349152\n","Lambda is: 0.01\n","Train set:  MSE=6.258270634244153 R2=0.9242082220920895\n","Test set:  MSE=17.272043729307587 R2=0.8053247970623901\n","Lambda is: 0.1\n","Train set:  MSE=11.45739562799781 R2=0.8612433952458572\n","Test set:  MSE=25.640899840151665 R2=0.7109984517110435\n","Lambda is: 1\n","Train set:  MSE=18.966995179357113 R2=0.770297200260362\n","Test set:  MSE=35.51343150683474 R2=0.5997240052216382\n","Lambda is: 10\n","Train set:  MSE=67.75245698455062 R2=0.17947313681353638\n","Test set:  MSE=74.68499614194536 R2=0.15821676877431756\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.799e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.537e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.820e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n"]}],"source":["from sklearn.linear_model import ElasticNet\n","\n","alphas = [0.0001, 0.001, 0.01, 0.1 ,1 ,10]\n","\n","for alpha in alphas:\n","    print(\"Lambda is: \"+str(alpha))\n","    elastic = ElasticNet(alpha=alpha, l1_ratio=0.5)\n","    elastic.fit(X_train_poly, y_train)\n","    overfit_eval(elastic, (X_train_poly, X_test_poly),(y_train, y_test))"]},{"cell_type":"markdown","metadata":{"id":"q4HgaVLmJKPM"},"source":["Utilizzando ElasticNet, e quindi entrambe le regolarizzazioni, abbiamo ottenuto un modello ancora migliore, con un R2 di 0.81 sul test set e 0.92 sul test set.<br>\n","Abbiamo il nostro vincitore!"]},{"cell_type":"markdown","metadata":{"id":"hambjO8SJKPM"},"source":["## Che differenza c'è tra la regolarizzazione L2 ed L1 ?"]},{"cell_type":"markdown","metadata":{"id":"-Y6nOzOIJKPM"},"source":["La differenza principale tra le due tecniche di regolarizzazione viste è la seguente:\n","* La regolarizzazione L2 riduce la magnitudine dei pesi a valori più bassi.\n","* La regolarizzazione L1 elimina le feature più deboli portando il loro peso a 0.\n","Nella pratica la L2 porta quasi sempre a migliori risultati, ma utilizzarle entrambe con ElasticNet è anche un ottima soluzione."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}