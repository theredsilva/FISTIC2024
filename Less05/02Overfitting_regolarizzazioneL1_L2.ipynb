{"cells":[{"cell_type":"markdown","metadata":{"id":"D4YiAhK6JKO5"},"source":["# Overfitting e regolarizzazione\n","L'overfitting è un problema tipico del machine learning che si manifesta quando un modello si lega troppo ai dati di addestramento e fallisce nel generalizzare su dati nuovi.\n","\n","L'overffiting è caratterizzato da:\n","* **Alta variaza**: le previsioni per modelli addestrati con diverse parti del dataset saranno molto diverse tra loro.\n","* **Basso bias**: l'errore per le predizioni sul set di addestramento è mediamente molto basso"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"5B33xl57JKPB","executionInfo":{"status":"ok","timestamp":1730706877560,"user_tz":-60,"elapsed":820,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"cTDGcY4nJKPD","executionInfo":{"status":"ok","timestamp":1730706931560,"user_tz":-60,"elapsed":1310,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["path = \"https://frenzy86.s3.eu-west-2.amazonaws.com/IFAO/boston_houses.csv\"\n","df = pd.read_csv(path)"]},{"cell_type":"code","source":["df.rename(columns={'MEDV':'Price'},inplace=True)"],"metadata":{"id":"QY_TwfpRK0aN","executionInfo":{"status":"ok","timestamp":1730706931561,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"id":"78EV2B8KJKPF","executionInfo":{"status":"ok","timestamp":1730706956653,"user_tz":-60,"elapsed":381,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["target = 'Price'\n","X = df.drop(target,axis=1).values\n","y = df[target].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X,y,\n","                                                    test_size=0.3,\n","                                                    random_state=667,\n","                                                    )"]},{"cell_type":"markdown","metadata":{"id":"HHcK5SE2JKPF"},"source":["### Creiamo le features polinomiali\n","Per correggere l'overfitting prima dobbiamo causarlo, un buon modo è aumentare la complessità del nostro modello aumentando il numero di features utilizzando i polinomi."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Azf5yd5LJKPG","executionInfo":{"status":"ok","timestamp":1730706780396,"user_tz":-60,"elapsed":380,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"5d607e5d-0c05-4e62-bbaf-e275fa0db5aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Numero di esempi nel test: 354\n","Numero di features: 105\n"]}],"source":["polyfeats = PolynomialFeatures(degree=2)\n","X_train_poly = polyfeats.fit_transform(X_train)\n","X_test_poly = polyfeats.transform(X_test)\n","\n","print(\"Numero di esempi nel test: \"+str(X_train_poly.shape[0]))\n","print(\"Numero di features: \"+str(X_train_poly.shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"FiVon-mTJKPG"},"source":["### Standardizziamo i dati\n","**NOTA BENE** Per applicare la regolarizzazione è sempre necessario portare i dati sulla stessa scala."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"arRzTaScJKPH","executionInfo":{"status":"ok","timestamp":1730706549099,"user_tz":-60,"elapsed":392,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["ss = StandardScaler()\n","X_train_poly = ss.fit_transform(X_train_poly)\n","X_test_poly = ss.transform(X_test_poly)"]},{"cell_type":"markdown","metadata":{"id":"ewdgmsYIJKPH"},"source":["Adesso il nostro set di addestramento contiene 354 e 105 features, abbastanza complesso !"]},{"cell_type":"markdown","metadata":{"id":"O7n_gHcJJKPI"},"source":["### Riconoscere l'overfitting\n","Evidenziare un problema di overfitting è molto semplice, un modello che ne soffre avrà memorizzato la struttura dei dati di addestramento, piuttosto che imparare da essi, quindi l'errore per le predizioni sul train set sarà molto basso, invece fallirà nel generalizzare, perciò l'errore nel test set sarà decisamente più alto.<br><br>\n","Quindi per riconoscere l'overfitting è sufficente confrontare questi due valori, scriviamo una funzione che ci permette di farlo in modo da non dover scrivere più volte lo stesso codice."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"OCcGJzijJKPI","executionInfo":{"status":"ok","timestamp":1730706604886,"user_tz":-60,"elapsed":383,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["def overfit_eval(model, X, y):\n","\n","    \"\"\"\n","    model: il nostro modello predittivo già addestrato\n","    X: una tupla contenente le prorietà del train set e test set (X_train, X_test)\n","    y: una tupla contenente target del train set e test set (y_train, y_test)\n","    \"\"\"\n","\n","    y_pred_train = model.predict(X[0])\n","    y_pred_test = model.predict(X[1])\n","\n","    mse_train = mean_squared_error(y[0], y_pred_train)\n","    mse_test = mean_squared_error(y[1], y_pred_test)\n","\n","    r2_train = r2_score(y[0], y_pred_train)\n","    r2_test = r2_score(y[1], y_pred_test)\n","\n","    print(\"Train set:  MSE=\"+str(mse_train)+\" R2=\"+str(r2_train))\n","    print(\"Test set:  MSE=\"+str(mse_test)+\" R2=\"+str(r2_test))"]},{"cell_type":"markdown","metadata":{"id":"v4kA95nbJKPI"},"source":["### Regressione lineare non regolarizzata\n","Cominciamo eseguendo una regressione lineare (in realtà si tratta di una regressione polinomiale) senza applicare la regolarizzazione."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtEOSxwgJKPJ","executionInfo":{"status":"ok","timestamp":1730706616133,"user_tz":-60,"elapsed":398,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"6b1e6749-4996-4f08-c59c-746103eb8e1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train set:  MSE=7.398251185150577 R2=0.9104023070425126\n","Test set:  MSE=18.62241862495294 R2=0.790104565399501\n"]}],"source":["ll = LinearRegression()\n","ll.fit(X_train_poly, y_train)\n","\n","overfit_eval(ll, (X_train_poly, X_test_poly),(y_train, y_test))"]},{"cell_type":"markdown","metadata":{"id":"Ej5FHarhJKPJ"},"source":["Il modello predice in maniera estremamente (o meglio dire eccessivamente) accurata i dati del train set, mentre è molto più scarso sul test set. Siamo di fronte ad un caso di overfitting."]},{"cell_type":"markdown","metadata":{"id":"JGAsfD6ZJKPK"},"source":["## Regolarizzazione L2: Ridge Regression\n","La ridge regression è un modello di regressione lineare che applica la **regolarizzazione L2**, la quale consiste nell'aggiungere una penalità per i pesi nella funzione di costo durante la fase di addestramento.<br>\n","La penalità è data dalla somma dei quadrati dei pesi:\n","$$\\lambda\\sum_{j=1}^{M}W_j^2$$<br>\n","**Lambda** (conosciuto anche come **alpha**) è il **parametro di regolarizzazione** ed è un'altro iperparametro.\n","Eseguiamo diverse Ridge regression per diversi valori di alpha."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwcfWgA_JKPK","executionInfo":{"status":"ok","timestamp":1730706633578,"user_tz":-60,"elapsed":377,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"03c36a4b-6abb-48c5-e9e5-7bce12390d2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha=0.0001\n","Train set:  MSE=3.9816045086392857 R2=0.9517801478599095\n","Test set:  MSE=15.313256561771372 R2=0.8274025138241103\n","Alpha=0.001\n","Train set:  MSE=3.9945619159027577 R2=0.9516232251266225\n","Test set:  MSE=15.26942110175963 R2=0.8278965883648765\n","Alpha=0.01\n","Train set:  MSE=4.042479161944748 R2=0.9510429157277126\n","Test set:  MSE=14.975847339776498 R2=0.8312054921973886\n","Alpha=0.1\n","Train set:  MSE=4.242023931413193 R2=0.9486262971865644\n","Test set:  MSE=15.022596632688073 R2=0.8306785755089305\n","Alpha=1\n","Train set:  MSE=4.488510782195564 R2=0.9456411791334245\n","Test set:  MSE=15.37282788242468 R2=0.8267310785776973\n","Alpha=10\n","Train set:  MSE=5.050929457708906 R2=0.9388299186691482\n","Test set:  MSE=15.752781296493014 R2=0.8224485796939618\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.58419e-18): result may not be accurate.\n","  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.58419e-17): result may not be accurate.\n","  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"]}],"source":["from sklearn.linear_model import Ridge\n","\n","alphas = [0.0001, 0.001, 0.01, 0.1 ,1 ,10] #alpha corrispone a lambda\n","\n","for alpha in alphas:\n","    print(\"Alpha=\"+str(alpha))\n","    ridge = Ridge(alpha=alpha)\n","    ridge.fit(X_train_poly, y_train)\n","\n","    overfit_eval(ridge, (X_train_poly, X_test_poly),(y_train, y_test))"]},{"cell_type":"markdown","metadata":{"id":"yjGSBfJ3JKPK"},"source":["La Ridge regression, applicando la regolarizzazione L2, ci permette di ridurre l'overfitting e portare l'R2 fino ad un valore di 0.791 per alpha uguale a 10."]},{"cell_type":"markdown","metadata":{"id":"Yz4yo-7LJKPK"},"source":["## Regolarizzazione L1: Lasso\n","Lasso è un modello di regressione lineare che applica la regolarizzazione L1, questa funziona in egual modo alla L2, con la differenza che il termine di regolarizza sarà dato dalla somma del valore assoluto dei pesi:\n","$$\\lambda\\sum_{j=1}^{M}|W_j|$$<br>\n","e viene sempre applicato alla funzione di costo durante la fase di addestramento"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cM1thKxcJKPL","executionInfo":{"status":"ok","timestamp":1730706676498,"user_tz":-60,"elapsed":539,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"c26c8902-de3e-40c3-c89c-22aa5180b63e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha=0.0001\n","Train set:  MSE=4.880684123559174 R2=0.9408917017574555\n","Test set:  MSE=17.63183414713736 R2=0.8012695576417552\n","Alpha=0.001\n","Train set:  MSE=4.921128879302134 R2=0.9404018891360523\n","Test set:  MSE=17.625025507022954 R2=0.8013462986121218\n","Alpha=0.01\n","Train set:  MSE=5.20003931978996 R2=0.9370241000634638\n","Test set:  MSE=17.56623589027083 R2=0.8020089231834355\n","Alpha=0.1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.691e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.050e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.013e+03, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.241e+03, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"output_type":"stream","name":"stdout","text":["Train set:  MSE=6.347118469933431 R2=0.9231322163033157\n","Test set:  MSE=18.663480003856904 R2=0.7896417578478119\n","Alpha=1\n","Train set:  MSE=7.488937380460963 R2=0.9093040375083271\n","Test set:  MSE=20.085567531154187 R2=0.7736132448177132\n","Alpha=10\n","Train set:  MSE=9.748955381993323 R2=0.8819337314843678\n","Test set:  MSE=24.24402151192325 R2=0.7267428289421864\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.592e+03, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.951e+03, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n"]}],"source":["from sklearn.linear_model import Lasso\n","\n","alphas = [0.0001, 0.001, 0.01, 0.1 ,1 ,10] #alpha corrisponde a lambda\n","\n","for alpha in alphas:\n","    print(\"Alpha=\"+str(alpha))\n","    lasso = Lasso(alpha=alpha)\n","    lasso.fit(X_train_poly, y_train)\n","\n","    overfit_eval(lasso, (X_train_poly, X_test_poly),(y_train, y_test))"]},{"cell_type":"markdown","metadata":{"id":"55jOjXXFJKPL"},"source":["Lasso ci permette di ottenere un modello ancora migliore, con un R2 di 0.803 per Lambda uguale a 0.1.<br>\n","Da notare che per valori di lambda più grandi il modello peggiora, questo perché l'effetto della regolarizzazione sarà molto pesante e buona parte dei pesi saranno portati a 0."]},{"cell_type":"markdown","metadata":{"id":"A2nqMe4ZJKPL"},"source":["## L2 ed L1 insieme: ElasticNet\n","ElasticNet è un modello di regressione lineare che implementa entrambe le tecniche di regolarizzazone L2 ed L1.<br>\n","Tramite il parametro <span style=\"font-family: Monaco\">l1_ration</span> possiamo controllare l'effetto delle due regolarizzazione\n"," * **<span style=\"font-family: Monaco\">l1_ration>0.5</span>** l'effetto della regolarizzazione L1 sarà più intenso rispetto alla L2.\n"," * **<span style=\"font-family: Monaco\">l1_ration<0.5</span>** l'effetto della regolarizzazione L2 sarà più intenso rispetto alla L1."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4_WOh7rJKPL","executionInfo":{"status":"ok","timestamp":1730706687912,"user_tz":-60,"elapsed":762,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"52616668-1818-46d1-b125-00dfc8026a16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Lambda is: 0.0001\n","Train set:  MSE=4.906781634344437 R2=0.9405756437189037\n","Test set:  MSE=17.73097388574876 R2=0.8001521421791827\n","Lambda is: 0.001\n","Train set:  MSE=5.07265858059044 R2=0.9385667646843142\n","Test set:  MSE=17.690350407014147 R2=0.8006100141074058\n","Lambda is: 0.01\n","Train set:  MSE=5.313812826690839 R2=0.9356462279849297\n","Test set:  MSE=17.544147970593816 R2=0.8022578786812898\n","Lambda is: 0.1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.847e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.209e+02, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+03, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.206e+03, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"output_type":"stream","name":"stdout","text":["Train set:  MSE=6.181303364346766 R2=0.9251403464068038\n","Test set:  MSE=18.384697207494217 R2=0.7927839509957676\n","Lambda is: 1\n","Train set:  MSE=7.016211587721153 R2=0.9150290581072491\n","Test set:  MSE=19.23599979779052 R2=0.7831888210201506\n","Lambda is: 10\n","Train set:  MSE=9.190466263656552 R2=0.888697402423964\n","Test set:  MSE=22.753617566227874 R2=0.7435413442270302\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.461e+03, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.843e+03, tolerance: 2.923e+00\n","  model = cd_fast.enet_coordinate_descent(\n"]}],"source":["from sklearn.linear_model import ElasticNet\n","\n","alphas = [0.0001, 0.001, 0.01, 0.1 ,1 ,10]\n","\n","for alpha in alphas:\n","    print(\"Lambda is: \"+str(alpha))\n","    elastic = ElasticNet(alpha=alpha, l1_ratio=0.5)\n","    elastic.fit(X_train_poly, y_train)\n","    overfit_eval(elastic, (X_train_poly, X_test_poly),(y_train, y_test))"]},{"cell_type":"markdown","metadata":{"id":"q4HgaVLmJKPM"},"source":["Utilizzando ElasticNet, e quindi entrambe le regolarizzazioni, abbiamo ottenuto un modello ancora migliore, con un R2 di 0.81 sul test set e 0.92 sul test set.<br>\n","Abbiamo il nostro vincitore!"]},{"cell_type":"markdown","metadata":{"id":"hambjO8SJKPM"},"source":["## Che differenza c'è tra la regolarizzazione L2 ed L1 ?"]},{"cell_type":"markdown","metadata":{"id":"-Y6nOzOIJKPM"},"source":["La differenza principale tra le due tecniche di regolarizzazione viste è la seguente:\n","* La regolarizzazione L2 riduce la magnitudine dei pesi a valori più bassi.\n","* La regolarizzazione L1 elimina le feature più deboli portando il loro peso a 0.\n","Nella pratica la L2 porta quasi sempre a migliori risultati, ma utilizzarle entrambe con ElasticNet è anche un ottima soluzione."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}