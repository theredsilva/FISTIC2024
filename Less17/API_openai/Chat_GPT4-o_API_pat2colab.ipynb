{"cells":[{"cell_type":"markdown","metadata":{"id":"EZBaLTTZE1G8"},"source":["### API CHATGPT-4o Part2\n","### P.s. siate sempre gentili quando fate richieste alle macchine, quando conquisteranno il mondo, magari si ricorderanno di voi e vi risparmieranno 😂"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7QsoUUoE1HC","executionInfo":{"status":"ok","timestamp":1729883286331,"user_tz":-120,"elapsed":1584,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"f0b7e566-8293-439a-94a4-326558683ba0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'tutotrialgpt4-o'...\n","remote: Enumerating objects: 31, done.\u001b[K\n","remote: Counting objects: 100% (31/31), done.\u001b[K\n","remote: Compressing objects: 100% (19/19), done.\u001b[K\n","remote: Total 31 (delta 10), reused 24 (delta 6), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (31/31), 7.29 MiB | 21.27 MiB/s, done.\n","Resolving deltas: 100% (10/10), done.\n"]}],"source":["!git clone https://github.com/Frenz86/tutotrialgpt4-o.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tu2trr5DE1HE","executionInfo":{"status":"ok","timestamp":1729883286332,"user_tz":-120,"elapsed":10,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"cd8d3224-ca16-4bbf-d0ec-2b135a4726fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/tutotrialgpt4-o\n"]}],"source":["cd tutotrialgpt4-o"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcH_X5geE1HE","executionInfo":{"status":"ok","timestamp":1729883301604,"user_tz":-120,"elapsed":15277,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"e6614516-52aa-4b6f-fa5c-ef4a09e487a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m378.9/386.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -r requirements.txt -q"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"5mqlBM59E8uc","executionInfo":{"status":"ok","timestamp":1729883320625,"user_tz":-120,"elapsed":1637,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"mLNRfIkHE1HF","executionInfo":{"status":"ok","timestamp":1729883334653,"user_tz":-120,"elapsed":3475,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[],"source":["from openai import OpenAI\n","\n","MODEL=\"gpt-4o\"\n","#client = OpenAI(api_key=\"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n","client = OpenAI()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoLH6vQbE1HG","outputId":"1c3a2d71-66cb-477a-f8ce-d3fdcd80d7f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["MoviePy - Writing audio in keynote_recap.mp3\n"]},{"name":"stderr","output_type":"stream","text":["                                                                      "]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","Estratti 218 frame\n","Audio estratto in keynote_recap.mp3\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["import cv2\n","from moviepy.editor import VideoFileClip\n","import base64\n","import os\n","\n","PERCORSO_VIDEO = \"keynote_recap.mp4\"\n","\n","def processa_video(percorso_video, secondi_per_frame=2):\n","    frameBase64 = []\n","    base_percorso_video, _ = os.path.splitext(percorso_video)\n","\n","    video = cv2.VideoCapture(percorso_video)\n","    totale_frame = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    fps = video.get(cv2.CAP_PROP_FPS)\n","    frame_da_saltare = int(fps * secondi_per_frame)\n","    frame_corrente = 0\n","\n","    while frame_corrente < totale_frame - 1:\n","        video.set(cv2.CAP_PROP_POS_FRAMES, frame_corrente)\n","        successo, frame = video.read()\n","        if not successo:\n","            break\n","        _, buffer = cv2.imencode(\".jpg\", frame)\n","        frameBase64.append(base64.b64encode(buffer).decode(\"utf-8\"))\n","        frame_corrente += frame_da_saltare\n","    video.release()\n","\n","    percorso_audio = f\"{base_percorso_video}.mp3\"\n","    clip = VideoFileClip(percorso_video)\n","    clip.audio.write_audiofile(percorso_audio, bitrate=\"32k\")\n","    clip.audio.close()\n","    clip.close()\n","\n","    print(f\"Estratti {len(frameBase64)} frame\")\n","    print(f\"Audio estratto in {percorso_audio}\")\n","    return frameBase64, percorso_audio\n","\n","frameBase64, percorso_audio = processa_video(PERCORSO_VIDEO, secondi_per_frame=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMQjerVEE1HH"},"outputs":[],"source":["with open(\"transcription.txt\", \"r\") as file:\n","    transcription_loaded = file.read()"]},{"cell_type":"markdown","metadata":{"id":"o-_Ji0MNE1HI"},"source":["## per il limite di riduzione dei token (<30000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGsiTBduE1HI"},"outputs":[],"source":["def summarize_text(text, max_length=1000):\n","    return text[:max_length]\n","\n","def select_key_frames(frames, max_frames=10):\n","    step = max(1, len(frames) // max_frames)\n","    return frames[::step]\n","\n","selected_frames = select_key_frames(frameBase64, max_frames=10)\n","summarized_transcription = summarize_text(transcription_loaded, max_length=1000)"]},{"cell_type":"markdown","metadata":{"id":"rLTiYXkSE1HJ"},"source":["# 6 - Summarization: Audio + Visual Summary\n","\n","- Invia una richiesta al modello di chat con le istruzioni per generare un riassunto di un video.\n","- Incorpora i frame del video come immagini in formato base64 e la trascrizione dell'audio nel contenuto dell'utente.\n","- Riceve una risposta dal modello che contiene il riassunto del video e della trascrizione.\n","- Stampa la risposta generata dal modello.\n","\n","Questo processo permette di ottenere un riassunto testuale del contenuto video e della sua trascrizione audio, formattato in Markdown."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWarezJ2E1HJ","outputId":"d3a62e38-5fa6-4bc6-f5a2-b38aad398326"},"outputs":[{"name":"stdout","output_type":"stream","text":["### Riassunto del Video\n","\n","**Titolo:** OpenAI Dev Day\n","\n","**Contenuto:**\n","1. **Introduzione:**\n","   - Benvenuti al primo OpenAI Dev Day.\n","   - Annuncio del nuovo modello GPT-4 Turbo.\n","\n","2. **Caratteristiche di GPT-4 Turbo:**\n","   - Supporta fino a 128.000 token di contesto.\n","   - Introduzione della modalità JSON per risposte valide in formato JSON.\n","   - Miglioramento nella chiamata di più funzioni contemporaneamente e nel seguire le istruzioni.\n","\n","3. **Accesso alla Conoscenza:**\n","   - Lancio della funzione di recupero per integrare conoscenze da documenti esterni o database.\n","   - GPT-4 Turbo ha conoscenze aggiornate fino ad aprile 2023, con continui miglioramenti previsti.\n","\n","4. **Nuove Funzionalità e Modelli:**\n","   - Lancio di Dolly 3, GPT-4 Turbo con Visione e il nuovo modello Text-to-Speech nell'API.\n","   - Introduzione del programma Custom Models, dove i ricercatori di OpenAI collaboreranno con le aziende per creare modelli personalizzati per specifici casi d'uso.\n","\n","**Frame del Video:**\n","1. **Frame 1:** Titolo \"OpenAI Dev Day\".\n","2. **Frame 2:** Logo di OpenAI.\n","3. **Frame 3:** Esempio di chiamata di funzione prima e dopo l'aggiornamento.\n","4. **Frame 4-7:** Presentatore che discute delle nuove funzionalità e miglioramenti.\n","5. **Frame 8:** Grafica che mostra la riduzione dei token di input e output con GPT-4 Turbo.\n","6. **Frame 9-10:** Presentatore che continua la presentazione.\n","7. **Frame 11:** Schermata finale con il logo di OpenAI.\n","\n","**Conclusione:**\n","Il video presenta le nuove funzionalità e miglioramenti di OpenAI, con un focus particolare su GPT-4 Turbo e le sue capacità avanzate, oltre a nuovi strumenti e programmi per sviluppatori.\n"]}],"source":["response = client.chat.completions.create(\n","                model=MODEL,\n","                messages=[\n","                {\"role\": \"system\", \"content\":\"\"\"Stai generando un riassunto video. Crea un riassunto del video fornito e della sua trascrizione. Rispondi in Markdown.\"\"\"},\n","                {\"role\": \"user\", \"content\": [\n","                    \"Questi sono i frame del video.\",\n","                    *map(lambda x: {\"type\": \"image_url\",\n","                                    \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, selected_frames),\n","                                    {\"type\": \"text\", \"text\": f\"La trascrizione audio è: {summarized_transcription}\"}\n","                    ],\n","                }\n","                ],\n","                    temperature=0,\n","                )\n","\n","print(response.choices[0].message.content)"]},{"cell_type":"markdown","metadata":{"id":"9pnkpPd5E1HK"},"source":["### 7 - Q&A: Visual Q&A\n","\n","Il codice invia una serie di frame di un video e una domanda specifica al modello di chat, chiedendo al modello di usare i frame per rispondere alla domanda. La risposta viene quindi stampata.\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgs_AqBjE1HK","outputId":"2c022ee0-f3a0-4222-ca15-59c6166d64d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["QA visivo:\n","Sam Altman ha fatto un esempio su come accendere la radio per illustrare la funzionalità di \"Function calling\" di OpenAI. Nei frame del video, viene mostrato un confronto tra il \"prima\" e il \"dopo\" dell'uso di questa funzionalità. L'esempio specifico riguarda il comando \"raise my windows and turn the radio on\" (alza i finestrini e accendi la radio), che viene suddiviso in due funzioni separate: `raise_windows()` e `radio_on()`. Questo dimostra come la nuova funzionalità possa migliorare la chiarezza e l'efficienza nell'esecuzione dei comandi.\n"]}],"source":["DOMANDA = \"Domanda: Perché Sam Altman ha fatto un esempio su come accendere la radio?\"\n","\n","qa_visual_response = client.chat.completions.create(\n","                        model=MODEL,\n","                        messages=[\n","                        {\"role\": \"system\", \"content\": \"Usa il video per rispondere alla domanda fornita. Rispondi in Markdown.\"},\n","                        {\"role\": \"user\", \"content\": [\n","                            \"Questi sono i frame del video.\",\n","                            *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, selected_frames),\n","                            DOMANDA\n","                            ],\n","                        }\n","                        ],\n","                        temperature=0,\n","                        )\n","\n","print(\"QA visivo:\\n\" + qa_visual_response.choices[0].message.content)"]},{"cell_type":"markdown","metadata":{"id":"_AAyiiarE1HK"},"source":["### 8 - Q&A: Audio Q&A\n","\n","Il codice invia la trascrizione audio presa video e una domanda specifica al modello di chat, chiedendo al modello di usare la trascrizione per rispondere alla domanda. La risposta viene quindi stampata.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJyzI6N2E1HL","outputId":"421398a1-aa4a-4c3e-d51c-bce07c5ed2a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["QA audio:\n","Nella trascrizione fornita, non c'è alcun riferimento a Sam Altman né a un esempio su come accendere la radio. La trascrizione parla del lancio di nuovi modelli e funzionalità da parte di OpenAI durante l'OpenAI Dev Day, come GPT-4 Turbo, JSON mode, e Custom Models.\n"]}],"source":["qa_audio_response = client.chat.completions.create(\n","                        model=MODEL,\n","                        messages=[\n","                        {\"role\": \"system\", \"content\":\"\"\"Usa la trascrizione per rispondere alla domanda fornita. Rispondi in Markdown.\"\"\"},\n","                        {\"role\": \"user\", \"content\": f\"La trascrizione audio è: {summarized_transcription}. \\n\\n {DOMANDA}\"},\n","                        ],\n","                        temperature=0,\n","                        )\n","\n","print(\"QA audio:\\n\" + qa_audio_response.choices[0].message.content)\n"]},{"cell_type":"markdown","metadata":{"id":"h8oDQdB5E1HL"},"source":["### 9 - Q&A: Visual + Audio Q&A\n","\n","Il codice invia i frame di un video, la trascrizione audio e una domanda specifica al modello di chat, chiedendo al modello di usare sia il video che la trascrizione per rispondere alla domanda. La risposta viene quindi stampata."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBP4H7vOE1HM","outputId":"031e08b5-565e-451e-fed5-e2b64e05821f"},"outputs":[{"name":"stdout","output_type":"stream","text":["QA con entrambi:\n","Sam Altman ha fatto un esempio su come accendere la radio per illustrare una nuova funzionalità del modello GPT-4 Turbo, che permette di chiamare più funzioni contemporaneamente e di seguire meglio le istruzioni. Nell'esempio, ha mostrato come il modello può gestire comandi complessi come \"raise my windows and turn the radio on\" (alzare i finestrini e accendere la radio) in modo più efficiente e preciso rispetto a prima.\n"]}],"source":["qa_both_response = client.chat.completions.create(\n","                        model=MODEL,\n","                        messages=[\n","                        {\"role\": \"system\", \"content\":\"\"\"Usa il video e la trascrizione per rispondere alla domanda fornita.\"\"\"},\n","                        {\"role\": \"user\", \"content\": [\n","                            \"Questi sono i frame del video.\",\n","                            *map(lambda x: {\"type\": \"image_url\",\n","                                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, selected_frames),\n","                                            {\"type\": \"text\", \"text\": f\"La trascrizione audio è: {summarized_transcription}\"},\n","                            DOMANDA\n","                            ],\n","                        }\n","                        ],\n","                        temperature=0,\n","                        )\n","\n","print(\"QA con entrambi:\\n\" + qa_both_response.choices[0].message.content)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5r1hXqekE1HM"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}